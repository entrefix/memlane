# RAG System Benchmark & Optimization Guide

**Document Version**: 1.0
**Last Updated**: 2025-12-31
**Author**: System Analysis
**Status**: Implementation Ready

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Current System Analysis](#current-system-analysis)
3. [Identified Issues & Bottlenecks](#identified-issues--bottlenecks)
4. [Benchmark Suite Design](#benchmark-suite-design)
5. [Implementation Roadmap](#implementation-roadmap)
6. [Optimization Strategies](#optimization-strategies)
7. [Expected Performance Gains](#expected-performance-gains)
8. [Technical Specifications](#technical-specifications)
9. [Testing & Validation](#testing--validation)
10. [Future Enhancements](#future-enhancements)

---

## Executive Summary

### Purpose

This document provides a comprehensive guide for implementing a benchmark suite to measure and optimize the RAG (Retrieval Augmented Generation) system used for memory search and retrieval in the Mr.Brain application.

### Goals

- **Measure** current system performance at 1K-10K memory scale
- **Identify** bottlenecks in indexing and search operations
- **Quantify** retrieval accuracy with precision/recall metrics
- **Implement** 5 high-impact optimizations
- **Validate** improvements with before/after comparisons

### Target Metrics

| Metric | Current (Est.) | Target | Improvement |
|--------|---------------|--------|-------------|
| Indexing Speed (5K) | 250-400s | 20-30s | **10-15x faster** |
| Throughput | 12-20 docs/s | 160-250 docs/s | **13x faster** |
| Search P95 Latency | 80-150ms | 60-80ms | **25-50% faster** |
| Precision@10 | 0.70-0.85 | 0.80-0.92 | **+5-10%** |
| API Call Reduction | 5,000 calls | 25 calls | **200x fewer** |

### Timeline

- **Week 1**: Build infrastructure + measure baseline (40 hours)
- **Week 2**: Implement optimizations + validate (40 hours)
- **Total**: 80-100 hours over 2 weeks

---

## Current System Analysis

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RAG System Architecture                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   Memory     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Embedding   â”‚                  â”‚
â”‚  â”‚   Service    â”‚         â”‚   Service    â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                        â”‚                           â”‚
â”‚         â”‚                        â”‚ OpenAI API                â”‚
â”‚         â”‚                        â”‚ text-embedding-3-small    â”‚
â”‚         â”‚                        â”‚ (1536 dimensions)         â”‚
â”‚         â–¼                        â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚         RAG Service                   â”‚                   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚
â”‚  â”‚  â”‚  Hybrid Search (RRF)           â”‚  â”‚                   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚                   â”‚
â”‚  â”‚  â”‚  â”‚  Vector  â”‚   â”‚  Keyword  â”‚ â”‚  â”‚                   â”‚
â”‚  â”‚  â”‚  â”‚  Search  â”‚   â”‚  Search   â”‚ â”‚  â”‚                   â”‚
â”‚  â”‚  â”‚  â”‚(chromem) â”‚   â”‚ (FTS5)    â”‚ â”‚  â”‚                   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚                   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚         â”‚                        â”‚                           â”‚
â”‚         â–¼                        â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  chromem-go â”‚         â”‚  SQLite     â”‚                    â”‚
â”‚  â”‚  Vector DB  â”‚         â”‚  FTS5       â”‚                    â”‚
â”‚  â”‚ (Persistent)â”‚         â”‚  Index      â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

#### 1. Vector Database: chromem-go (v0.7.0)

**File**: `backend/internal/repository/vector_repository.go`

**Features**:
- In-memory and persistent storage modes
- Thread-safe with RWMutex
- Document cache for quick lookups
- Metadata filtering support

**Limitations**:
- O(n) linear search (no ANN index like HNSW)
- Full collection scan for every query
- Memory usage grows linearly with collection size

**Storage Location**: `./data/vectors/`

#### 2. Embedding Service: OpenAI API

**File**: `backend/internal/services/embedding_service.go`

**Configuration**:
- Default Model: `text-embedding-3-small` (1536 dimensions)
- Alternative: `text-embedding-3-large` (3072 dimensions)
- Max Input: 8000 characters (truncated)
- Batch Support: Yes (via `EmbedBatch()`)

**Current Behavior**:
- Individual API calls per document during indexing
- Text preprocessing: newline replacement, truncation
- No retry logic or rate limiting

#### 3. Hybrid Search: Reciprocal Rank Fusion

**File**: `backend/internal/services/rag_service.go`

**Algorithm**:
```
RRF Score = Î£(weight_i / (k + rank_i))
where:
  - weight_vector = 0.7 (default)
  - weight_keyword = 0.3
  - k = 60 (RRF constant)
  - rank_i = position in result list
```

**Search Flow**:
1. Vector search (parallel goroutine)
2. Keyword search (parallel goroutine)
3. Merge with RRF scoring
4. Sort by combined score
5. Limit to top K results
6. Enrich with full document data

#### 4. Full-Text Search: SQLite FTS5

**File**: `backend/internal/repository/fts_repository.go`

**Features**:
- Porter Unicode61 tokenizer
- Auto-sync triggers (insert/update/delete)
- Snippet generation with highlighting
- Virtual table: `content_fts`

**Schema**:
```sql
CREATE VIRTUAL TABLE content_fts USING fts5(
    content_id,
    content_type,
    user_id UNINDEXED,
    title,
    content,
    tags,
    category,
    tokenize='porter unicode61'
);
```

### Data Flow

#### Indexing Flow

```
Memory Created
      â”‚
      â–¼
Memory Service
      â”‚ (async goroutine, 10s timeout)
      â–¼
RAG Service.IndexMemory()
      â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼             â–¼              â–¼
  Convert to    Generate      Update
  Document     Embedding      Metadata
      â”‚             â”‚              â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
          Vector Repository.Add()
                    â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼                           â–¼
chromem-go Collection        SQLite FTS5
 (vector storage)          (auto-trigger)
```

#### Search Flow

```
User Query
      â”‚
      â–¼
RAG Service.Search()
      â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                â”‚                 â”‚
      â–¼                â–¼                 â–¼
Generate Query     Vector Search    Keyword Search
 Embedding        (chromem-go)        (FTS5)
                       â”‚                 â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                      Reciprocal Rank Fusion
                                 â”‚
                                 â–¼
                         Sort & Limit Results
                                 â”‚
                                 â–¼
                         Enrich with Full Data
                                 â”‚
                                 â–¼
                         Return SearchResponse
```

---

## Identified Issues & Bottlenecks

### Critical Issues

#### 1. ðŸš¨ Hard-Coded 1000 Memory Limit (BLOCKER)

**Location**: `backend/internal/services/rag_service.go:417`

**Code**:
```go
memories, err := s.memoryRepo.GetAllByUserID(userID, 1000, 0)
```

**Impact**:
- Complete blocker for 5K-10K scale testing
- Users with >1000 memories have incomplete search results
- No pagination or batching logic

**Priority**: CRITICAL - Must fix before any scale testing

---

#### 2. âš¡ Sequential Embedding API Calls (PERFORMANCE)

**Location**: `backend/internal/repository/vector_repository.go:140-190`

**Current Behavior**:
```go
// In AddBatch() - WRONG: Individual API calls
for _, doc := range docs {
    embedding := r.embeddingFn(ctx, doc.Content)  // Each doc = 1 API call
}
```

**Impact**:
- 5K memories = 5,000 API calls
- Each call: ~50-100ms network latency
- Total: 250-500s indexing time
- 5,000x OpenAI billing units

**Expected with Batching**:
- 5K memories = 25 API calls (batch size 200)
- Total: 15-30s indexing time
- 25x billing units (200x reduction)

**Priority**: CRITICAL - Highest performance impact

---

#### 3. ðŸŽ¯ No Relevance Score Threshold (ACCURACY)

**Location**: `backend/internal/services/rag_service.go:130-182`

**Issue**:
- Returns results regardless of similarity score
- Low-quality matches (score < 0.3) dilute result quality
- No way to filter "not relevant" results

**Example**:
```
Query: "machine learning projects"
Results:
  1. ML project notes (score: 0.92) âœ… Relevant
  2. AI research paper (score: 0.81) âœ… Relevant
  3. Team lunch menu (score: 0.23) âŒ Irrelevant
  4. Random meeting (score: 0.19) âŒ Irrelevant
```

**Priority**: HIGH - Impacts user experience

---

#### 4. âš¡ Sequential Multi-Type Search (LATENCY)

**Location**: `backend/internal/repository/vector_repository.go:250-266`

**Current Code**:
```go
// WRONG: Sequential queries
for _, ct := range contentTypes {
    filters["content_type"] = ct
    results, err := r.Search(ctx, query, limit, filters)
    allResults = append(allResults, results...)
}
```

**Impact**:
- 2 content types: 2x search latency (100ms â†’ 200ms)
- 3 content types: 3x search latency (100ms â†’ 300ms)

**Priority**: MEDIUM - Affects common use case

---

#### 5. ðŸ“Š Silent Text Truncation (OBSERVABILITY)

**Location**: `backend/internal/services/embedding_service.go:107`

**Code**:
```go
if len(text) > 8000 {
    text = text[:8000]  // Silent truncation
}
```

**Issue**:
- No logging when content is truncated
- Unknown how often this occurs
- Lost context for long articles/documents

**Priority**: LOW - Observability improvement

---

### Performance Characteristics

#### Indexing Performance (Current)

| Memories | Embedding API Time | Storage Time | Total Time | Throughput |
|----------|-------------------|--------------|------------|------------|
| 100 | 5-10s | 0.5s | 5-10s | 10-20 docs/s |
| 1,000 | 50-100s | 5s | 55-105s | 9-18 docs/s |
| 5,000 | 250-500s | 25s | 275-525s | 9-18 docs/s |
| 10,000 | 500-1000s | 50s | 550-1050s | 9-18 docs/s |

**Bottleneck**: Embedding API calls (95% of time)

#### Search Performance (Current)

| Collection Size | Vector Search | Keyword Search | Hybrid | P95 Latency |
|-----------------|---------------|----------------|--------|-------------|
| 100 | 10-20ms | 5-10ms | 15-30ms | 35ms |
| 1,000 | 30-50ms | 10-20ms | 40-70ms | 85ms |
| 5,000 | 60-100ms | 20-40ms | 80-140ms | 155ms |
| 10,000 | 120-200ms | 40-80ms | 160-280ms | 310ms |

**Bottleneck**: Vector search linear scan (O(n))

---

## Benchmark Suite Design

### Architecture

```
backend/
â”œâ”€â”€ internal/
â”‚   â””â”€â”€ benchmark/
â”‚       â”œâ”€â”€ types.go           # Data structures
â”‚       â”œâ”€â”€ generator.go       # Synthetic data generation
â”‚       â”œâ”€â”€ dataset.go         # Dataset I/O
â”‚       â”œâ”€â”€ performance.go     # Performance benchmarks
â”‚       â”œâ”€â”€ accuracy.go        # Accuracy metrics
â”‚       â”œâ”€â”€ hybrid.go          # Hybrid search analysis
â”‚       â”œâ”€â”€ metrics.go         # Metric calculators
â”‚       â”œâ”€â”€ reporter.go        # Results export
â”‚       â””â”€â”€ fixtures/          # Test datasets
â”‚           â”œâ”€â”€ 1k_memories.json
â”‚           â”œâ”€â”€ 5k_memories.json
â”‚           â””â”€â”€ 10k_memories.json
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ benchmark/
â”‚       â”œâ”€â”€ main.go            # CLI entry point
â”‚       â””â”€â”€ commands/
â”‚           â”œâ”€â”€ generate.go    # Generate datasets
â”‚           â”œâ”€â”€ run.go         # Run benchmarks
â”‚           â”œâ”€â”€ baseline.go    # Baseline measurement
â”‚           â””â”€â”€ compare.go     # Compare results
â””â”€â”€ benchmark_results/         # Output directory
    â”œâ”€â”€ baseline_1k.json
    â”œâ”€â”€ baseline_5k.json
    â”œâ”€â”€ improved_5k.json
    â””â”€â”€ report.md
```

### Core Data Structures

#### BenchmarkConfig

```go
type BenchmarkConfig struct {
    UserID           string    // Test user ID
    DatasetSize      int       // 1000, 5000, 10000
    QueryCount       int       // Number of test queries (default: 100)
    DatasetType      string    // "synthetic", "replay", "mixed"
    Warmup           bool      // Pre-load caches before testing
    ExportFormat     string    // "json", "csv", "markdown"
    OutputPath       string    // Where to save results
}
```

#### TestDataset

```go
type TestDataset struct {
    Memories       []models.Memory    // Generated test memories
    Queries        []TestQuery        // Test queries with ground truth
    GroundTruth    map[string][]string // query_id -> relevant doc IDs
    Metadata       DatasetMetadata    // Dataset info
}

type TestQuery struct {
    ID             string    // Unique query ID
    Query          string    // Query text
    QueryType      string    // "keyword", "semantic", "hybrid"
    ExpectedDocs   []string  // Content IDs that should appear
    MinRelevantK   int       // Minimum relevant docs in top K
}

type DatasetMetadata struct {
    GeneratedAt    time.Time
    Size           int
    Categories     map[string]int  // Distribution
    AvgLength      int
    QueryTypes     map[string]int
}
```

#### Performance Metrics

```go
type PerformanceMetrics struct {
    Indexing       IndexingMetrics
    Search         SearchMetrics
    Memory         MemoryMetrics
}

type IndexingMetrics struct {
    TotalTime      time.Duration  // Total indexing time
    AvgPerDoc      time.Duration  // Average per document
    P50, P95, P99  time.Duration  // Latency percentiles
    DocsPerSecond  float64        // Throughput
    APICallsCount  int            // Total API calls made
    APITotalTime   time.Duration  // Time spent in API calls
    StorageTime    time.Duration  // Time spent storing vectors
}

type SearchMetrics struct {
    VectorSearch   LatencyStats   // Vector-only search
    KeywordSearch  LatencyStats   // Keyword-only search
    HybridSearch   LatencyStats   // Hybrid search
    QueriesPerSec  float64        // Query throughput
}

type LatencyStats struct {
    Mean, P50, P95, P99, Min, Max  time.Duration
    Count                          int
    Samples                        []time.Duration
}

type MemoryMetrics struct {
    VectorDBSize      int64  // Bytes on disk
    SQLiteDBSize      int64  // FTS5 index size
    PeakRAMUsage      int64  // Peak RSS
    AvgRAMPerDoc      int64  // RAM per document
}
```

#### Accuracy Metrics

```go
type AccuracyMetrics struct {
    PrecisionAtK   map[int]float64  // K=1,5,10,20
    RecallAtK      map[int]float64  // K=1,5,10,20
    MRR            float64          // Mean Reciprocal Rank
    NDCG           map[int]float64  // Normalized DCG @K
    ByQueryType    map[string]QueryTypeMetrics
}

type QueryTypeMetrics struct {
    QueryType         string
    TotalQueries      int
    Precision, Recall float64
    MRR               float64
    FailureRate       float64  // % with no relevant results
}
```

### Synthetic Data Generation

#### Memory Distribution

**Categories** (mirrors real usage):
- Personal (30%): Journal entries, thoughts, reflections
- Work (25%): Meeting notes, project updates, ideas
- Learning (20%): Articles, tutorials, course notes
- Reference (15%): Documentation, how-tos, code snippets
- Other (10%): Miscellaneous content

**Content Length Distribution**:
- Short (100-300 chars): 30%
- Medium (300-1000 chars): 50%
- Long (1000-3000 chars): 15%
- Very Long (3000-8000 chars): 5%

**Temporal Distribution** (created_at):
- Last week: 20%
- Last month: 30%
- Last 3 months: 30%
- Older: 20%

#### Query Generation Strategy

**Query Types**:

1. **Keyword Queries (40%)**:
   - Extract 2-3 word exact phrases from documents
   - Example: "machine learning project" (from doc content)

2. **Semantic Queries (40%)**:
   - Rephrase document content with synonyms
   - Example: Doc: "ML project", Query: "artificial intelligence work"

3. **Hybrid Queries (20%)**:
   - Combine specific keywords + general concepts
   - Example: "React component authentication flow"

#### Ground Truth Labeling

For each query, mark 3-5 documents as relevant using:

1. **Source document** (if query derived from it) - always relevant
2. **BM25 similarity** > 0.7 threshold
3. **Embedding similarity** > 0.8 threshold
4. **Manual curation** for edge cases

Include **hard negatives**: Documents similar in topic but not relevant to query.

### Metric Calculations

#### Precision@K

"What fraction of top K results are relevant?"

```
Precision@K = |{relevant docs} âˆ© {top K retrieved}| / K

Example:
  Query: "machine learning projects"
  Top 10 results: 8 relevant, 2 irrelevant
  Precision@10 = 8/10 = 0.80
```

#### Recall@K

"What fraction of all relevant docs are in top K?"

```
Recall@K = |{relevant docs} âˆ© {top K retrieved}| / |{all relevant docs}|

Example:
  Query: "machine learning projects"
  Total relevant docs in collection: 12
  Found in top 10: 8
  Recall@10 = 8/12 = 0.67
```

#### Mean Reciprocal Rank (MRR)

"How quickly do users find the first relevant result?"

```
RR = 1 / rank_of_first_relevant_doc
MRR = (1/|Q|) * Î£(RR_i) for all queries

Example:
  Query 1: First relevant at position 1 â†’ RR = 1.0
  Query 2: First relevant at position 3 â†’ RR = 0.33
  Query 3: First relevant at position 2 â†’ RR = 0.50
  MRR = (1.0 + 0.33 + 0.50) / 3 = 0.61
```

#### NDCG@K (Normalized Discounted Cumulative Gain)

"Quality-weighted ranking metric"

```
DCG@K = Î£(relevance_i / log2(i+1)) for i=1..K
NDCG@K = DCG@K / IDCG@K (ideal DCG)

Example:
  Top 5 results with relevance scores [3, 2, 3, 0, 1]:
  DCG@5 = 3/log2(2) + 2/log2(3) + 3/log2(4) + 0/log2(5) + 1/log2(6)
        = 3.0 + 1.26 + 1.5 + 0 + 0.39 = 6.15

  Ideal ranking [3, 3, 2, 1, 0]:
  IDCG@5 = 3.0 + 1.89 + 1.26 + 0.43 + 0 = 6.58

  NDCG@5 = 6.15 / 6.58 = 0.93
```

### Test Suites

#### 1. Performance Benchmarks

**Indexing Test**:
```go
func BenchmarkIndexing(cfg BenchmarkConfig, dataset TestDataset) IndexingMetrics {
    // Scenarios:
    // 1. Cold start: Index all from scratch
    // 2. Incremental: Add 100 docs to existing 5K
    // 3. Update: Modify 100 docs and re-index

    // Track:
    // - Total time
    // - Per-document latency
    // - API vs. storage time breakdown
    // - Memory usage during indexing

    return metrics
}
```

**Search Test**:
```go
func BenchmarkSearch(cfg BenchmarkConfig, dataset TestDataset) SearchMetrics {
    // Test modes:
    // 1. Vector-only (weight=1.0)
    // 2. Keyword-only (weight=0.0)
    // 3. Hybrid (weight=0.7)

    // Warmup: Run 10 queries first
    // Measurement: Run all 100 queries

    // Track:
    // - Latency distribution (p50, p95, p99)
    // - Queries per second
    // - Mode comparison

    return metrics
}
```

#### 2. Accuracy Benchmarks

```go
func BenchmarkAccuracy(cfg BenchmarkConfig, dataset TestDataset) AccuracyMetrics {
    metrics := AccuracyMetrics{
        PrecisionAtK: make(map[int]float64),
        RecallAtK:    make(map[int]float64),
        NDCG:         make(map[int]float64),
        ByQueryType:  make(map[string]QueryTypeMetrics),
    }

    for _, query := range dataset.Queries {
        // Run search
        results := ragService.Search(query.Query)
        groundTruth := dataset.GroundTruth[query.ID]

        // Calculate metrics for K=1,5,10,20
        for _, k := range []int{1, 5, 10, 20} {
            p, r := CalculatePrecisionRecall(results, groundTruth, k)
            metrics.PrecisionAtK[k] += p
            metrics.RecallAtK[k] += r
        }

        // Track by query type
        metrics.ByQueryType[query.QueryType].Update(results, groundTruth)
    }

    // Average across all queries
    metrics.Normalize(len(dataset.Queries))

    return metrics
}
```

#### 3. Hybrid Search Analysis

```go
func BenchmarkHybridModes(dataset TestDataset) HybridMetrics {
    // Test vector weights: 0.0, 0.1, 0.2, ..., 0.9, 1.0

    for weight := 0.0; weight <= 1.0; weight += 0.1 {
        // Run all queries with this weight
        accuracy := measureAccuracy(dataset, weight)
        latency := measureLatency(dataset, weight)

        // Track best weight
        if accuracy > bestAccuracy {
            optimalWeight = weight
        }
    }

    return HybridMetrics{
        VectorOnly:    results[1.0],
        KeywordOnly:   results[0.0],
        Hybrid:        results[0.7],
        OptimalWeight: optimalWeight,
    }
}
```

### CLI Commands

#### Generate Test Datasets

```bash
go run cmd/benchmark/main.go generate \
    --size 5000 \
    --queries 100 \
    --output internal/benchmark/fixtures/5k.json

# Options:
#   --size         Number of memories to generate
#   --queries      Number of test queries (default: 100)
#   --output       Output file path
#   --seed         Random seed for reproducibility
#   --categories   Custom category distribution (JSON)
```

#### Run Baseline Benchmark

```bash
go run cmd/benchmark/main.go baseline \
    --dataset internal/benchmark/fixtures/5k.json \
    --export json \
    --output benchmark_results/baseline_5k.json

# Options:
#   --dataset      Input dataset file
#   --export       Output format: json, csv, markdown
#   --output       Output file path
#   --warmup       Enable warmup run (default: true)
#   --verbose      Verbose logging
```

#### Run Specific Test Suite

```bash
# Performance only
go run cmd/benchmark/main.go run \
    --dataset internal/benchmark/fixtures/5k.json \
    --tests performance \
    --output benchmark_results/perf_5k.json

# Accuracy only
go run cmd/benchmark/main.go run \
    --dataset internal/benchmark/fixtures/5k.json \
    --tests accuracy \
    --output benchmark_results/acc_5k.json

# Hybrid analysis
go run cmd/benchmark/main.go run \
    --dataset internal/benchmark/fixtures/5k.json \
    --tests hybrid \
    --output benchmark_results/hybrid_5k.json
```

#### Compare Results

```bash
go run cmd/benchmark/main.go compare \
    --before benchmark_results/baseline_5k.json \
    --after benchmark_results/improved_5k.json \
    --output benchmark_results/comparison.md \
    --format markdown

# Output:
# - Side-by-side comparison table
# - Percentage improvements
# - Visual indicators (âœ… improved, âŒ regressed)
# - Statistical significance
```

### Output Formats

#### JSON Output

```json
{
  "config": {
    "dataset_size": 5000,
    "query_count": 100,
    "timestamp": "2025-01-15T10:30:00Z",
    "version": "1.0"
  },
  "performance": {
    "indexing": {
      "total_time_ms": 45000,
      "avg_per_doc_ms": 9,
      "p50_ms": 8,
      "p95_ms": 12,
      "p99_ms": 18,
      "docs_per_second": 111,
      "api_calls": 5000,
      "api_time_ms": 42500,
      "storage_time_ms": 2500
    },
    "search": {
      "hybrid_p50_ms": 45,
      "hybrid_p95_ms": 89,
      "hybrid_p99_ms": 125,
      "queries_per_second": 22
    },
    "memory": {
      "vector_db_mb": 450,
      "sqlite_db_mb": 120,
      "peak_ram_mb": 850
    }
  },
  "accuracy": {
    "precision_at_k": {
      "1": 0.95,
      "5": 0.91,
      "10": 0.87,
      "20": 0.81
    },
    "recall_at_k": {
      "1": 0.19,
      "5": 0.61,
      "10": 0.73,
      "20": 0.85
    },
    "mrr": 0.82,
    "ndcg": {
      "5": 0.84,
      "10": 0.87,
      "20": 0.89
    },
    "by_query_type": {
      "keyword": {
        "precision": 0.89,
        "recall": 0.75,
        "mrr": 0.85
      },
      "semantic": {
        "precision": 0.84,
        "recall": 0.71,
        "mrr": 0.79
      },
      "hybrid": {
        "precision": 0.88,
        "recall": 0.78,
        "mrr": 0.83
      }
    }
  }
}
```

#### Markdown Report

````markdown
# RAG Benchmark Report

**Dataset**: 5,000 memories
**Queries**: 100
**Date**: 2025-01-15 10:30:00
**Version**: Baseline

---

## Performance Metrics

### Indexing Performance

| Metric | Value | Unit |
|--------|-------|------|
| Total Time | 45.0 | seconds |
| Throughput | 111 | docs/sec |
| Avg Per Doc | 9 | ms |
| P50 Latency | 8 | ms |
| P95 Latency | 12 | ms |
| P99 Latency | 18 | ms |
| API Calls | 5,000 | calls |
| API Time | 42.5 | seconds |
| Storage Time | 2.5 | seconds |

**Analysis**: API calls dominate (94% of time). Batching would provide significant improvement.

### Search Performance

| Metric | Vector Only | Keyword Only | Hybrid | Unit |
|--------|-------------|--------------|--------|------|
| P50 Latency | 65 | 15 | 45 | ms |
| P95 Latency | 125 | 35 | 89 | ms |
| P99 Latency | 180 | 55 | 125 | ms |
| QPS | 15 | 66 | 22 | queries/sec |

**Analysis**: Hybrid provides best accuracy with acceptable latency.

### Memory Usage

| Metric | Value | Unit |
|--------|-------|------|
| Vector DB Size | 450 | MB |
| SQLite DB Size | 120 | MB |
| Peak RAM | 850 | MB |
| RAM per Doc | 0.17 | MB |

---

## Accuracy Metrics

### Precision & Recall

| K | Precision@K | Recall@K |
|---|-------------|----------|
| 1 | 0.95 | 0.19 |
| 5 | 0.91 | 0.61 |
| 10 | 0.87 | 0.73 |
| 20 | 0.81 | 0.85 |

**Mean Reciprocal Rank (MRR)**: 0.82

### By Query Type

| Query Type | Precision@10 | Recall@10 | MRR | Count |
|------------|--------------|-----------|-----|-------|
| Keyword | 0.89 | 0.75 | 0.85 | 40 |
| Semantic | 0.84 | 0.71 | 0.79 | 40 |
| Hybrid | 0.88 | 0.78 | 0.83 | 20 |

**Analysis**: Keyword queries perform best, semantic slightly lower but still good.

---

## Recommendations

1. **Implement batch embedding API calls** - Expected 10-15x speedup
2. **Add relevance threshold filtering** - Improve precision by 5-10%
3. **Parallelize multi-type search** - Reduce latency by 40-60%
4. **Monitor truncation events** - Better observability

---

## Test Configuration

- Dataset: `internal/benchmark/fixtures/5k.json`
- Environment: Development
- Go Version: 1.21
- chromem-go: v0.7.0
- OpenAI Model: text-embedding-3-small
````

---

## Implementation Roadmap

### Phase 1: Infrastructure (Week 1, Days 1-5)

#### Day 1: Core Framework (8 hours)

**Tasks**:
1. Create `backend/internal/benchmark/` directory
2. Implement `types.go`:
   - BenchmarkConfig
   - TestDataset
   - PerformanceMetrics
   - AccuracyMetrics
   - All supporting structs
3. Implement `metrics.go`:
   - CalculatePrecisionRecall()
   - CalculateMRR()
   - CalculateNDCG()
   - Percentile calculations
4. Write unit tests for metric calculators

**Deliverables**:
- Complete type definitions
- Tested metric calculation functions
- Unit test coverage >80%

**Dependencies**:
```bash
go get github.com/montanaflynn/stats  # Percentile calculations
```

---

#### Day 2-3: Data Generation (16 hours)

**Tasks**:
1. Implement `generator.go`:
   - GenerateMemories() with realistic distributions
   - GenerateQueries() with 3 types
   - CreateGroundTruth() with multiple methods
   - SaveDataset() for persistence
2. Implement `dataset.go`:
   - LoadDataset() from JSON
   - ValidateDataset() for sanity checks
3. Generate fixture files (1K, 5K, 10K)
4. Manual validation of sample data

**Deliverables**:
- Synthetic data generator
- 3 fixture datasets
- Validation report

**Dependencies**:
```bash
go get github.com/brianvoe/gofakeit/v6  # Realistic fake data
```

**Example Generator Usage**:
```go
dataset := benchmark.GenerateTestDataset(5000, benchmark.GeneratorConfig{
    QueryCount: 100,
    Categories: map[string]float64{
        "personal": 0.30,
        "work": 0.25,
        "learning": 0.20,
        "reference": 0.15,
        "other": 0.10,
    },
    Seed: 42,  // Reproducible
})
```

---

#### Day 4: Test Implementations (8 hours)

**Tasks**:
1. Implement `performance.go`:
   - BenchmarkIndexing()
   - BenchmarkSearch()
   - BenchmarkMemory()
   - Helper functions for timing
2. Implement `accuracy.go`:
   - BenchmarkAccuracy()
   - Query type breakdown
   - Failure analysis
3. Implement `hybrid.go`:
   - BenchmarkHybridModes()
   - Weight optimization

**Deliverables**:
- Complete benchmark implementations
- Integration with RAG service
- Timing helpers

---

#### Day 5: CLI & Reporting (8 hours)

**Tasks**:
1. Implement `cmd/benchmark/main.go`:
   - Cobra CLI setup
   - Commands: generate, run, baseline, compare
2. Implement `reporter.go`:
   - JSON export
   - CSV export
   - Markdown report generation
   - Console summary
3. Add progress bars and logging
4. End-to-end testing

**Deliverables**:
- Working CLI tool
- All export formats
- User documentation

**Dependencies**:
```bash
go get github.com/spf13/cobra          # CLI framework
go get github.com/spf13/viper          # Config management
go get github.com/olekukonko/tablewriter  # Markdown tables
go get github.com/schollz/progressbar/v3  # Progress bars
```

---

### Phase 2: Baseline Measurement (Week 1, Days 6-7)

#### Day 6: Run Benchmarks (6 hours)

**Tasks**:
1. Generate datasets if not already done
2. Run baseline for 1K scale
3. Run baseline for 5K scale
4. Run baseline for 10K scale
5. Collect system metrics (CPU, RAM, disk I/O)

**Commands**:
```bash
# Generate datasets
./benchmark generate --size 1000 --output fixtures/1k.json
./benchmark generate --size 5000 --output fixtures/5k.json
./benchmark generate --size 10000 --output fixtures/10k.json

# Run baselines
./benchmark baseline --dataset fixtures/1k.json --output results/baseline_1k.json
./benchmark baseline --dataset fixtures/5k.json --output results/baseline_5k.json
./benchmark baseline --dataset fixtures/10k.json --output results/baseline_10k.json
```

**Expected Results** (5K scale):
- Indexing: 250-400s
- Search P95: 80-150ms
- Precision@10: 0.70-0.85
- Recall@10: 0.60-0.75

---

#### Day 7: Analysis (10 hours)

**Tasks**:
1. Generate markdown reports for all scales
2. Identify bottlenecks:
   - Time breakdown (API vs. storage)
   - Latency hotspots
   - Accuracy weak points
3. Validate improvement priorities
4. Document findings
5. Create visualization charts (optional)

**Deliverables**:
- 3 markdown reports (1K, 5K, 10K)
- Bottleneck analysis document
- Confirmed optimization priorities

---

### Phase 3: Implement Optimizations (Week 2, Days 8-11)

#### Day 8 Morning: Fix #2 - Remove 1000 Limit (30 min)

**File**: `backend/internal/services/rag_service.go:416-436`

**Before**:
```go
// Index memories
memories, err := s.memoryRepo.GetAllByUserID(userID, 1000, 0)
if err != nil {
    log.Printf("[RAG] Error fetching memories: %v", err)
} else {
    for _, memory := range memories {
        // ... indexing logic
    }
}
```

**After**:
```go
// Index memories with pagination
const BATCH_SIZE = 1000
offset := 0

for {
    memories, err := s.memoryRepo.GetAllByUserID(userID, BATCH_SIZE, offset)
    if err != nil {
        log.Printf("[RAG] Error fetching memories at offset %d: %v", offset, err)
        break
    }

    if len(memories) == 0 {
        break  // No more memories
    }

    log.Printf("[RAG] Indexing batch of %d memories (offset: %d)", len(memories), offset)

    for _, memory := range memories {
        // Check if already indexed
        if s.vectorRepo.GetByContentID(models.ContentTypeMemory, memory.ID) != nil {
            skipped++
            continue
        }

        doc := s.memoryToDocument(&memory)
        if err := s.vectorRepo.Add(ctx, doc); err != nil {
            log.Printf("[RAG] Error indexing memory %s: %v", memory.ID, err)
            errors++
        } else {
            indexed++
        }
    }

    offset += BATCH_SIZE

    // Safety: If we got fewer than BATCH_SIZE, we're done
    if len(memories) < BATCH_SIZE {
        break
    }
}
```

**Testing**:
```bash
# Create test user with 5K memories
# Run indexing
# Verify all 5K are indexed
```

**Expected Impact**: âœ… Unblocks 5K-10K scale

---

#### Day 8 Afternoon: Fix #1 - Batch Embeddings (3 hours)

**File**: `backend/internal/repository/vector_repository.go:140-190`

**Current Issue**:
```go
// AddBatch() currently calls embedding individually
func (r *VectorRepository) AddBatch(ctx context.Context, docs []*models.Document) error {
    chromemDocs := make([]chromem.Document, len(docs))

    for i, doc := range docs {
        content := prepareContentForEmbedding(doc)
        // ... metadata setup ...

        chromemDocs[i] = chromem.Document{
            ID:       doc.ID,
            Content:  content,  // chromem-go will call embeddingFn per doc
            Metadata: metadata,
        }
    }

    // Each doc triggers individual API call here
    err := r.collection.AddDocuments(ctx, chromemDocs, runtime())
}
```

**Solution**: Pre-generate embeddings in batches

```go
// AddBatch() with batch embedding generation
func (r *VectorRepository) AddBatch(ctx context.Context, docs []*models.Document) error {
    r.mu.Lock()
    defer r.mu.Unlock()

    if len(docs) == 0 {
        return nil
    }

    const EMBED_BATCH_SIZE = 200

    // Prepare all documents and generate embeddings in batches
    chromemDocs := make([]chromem.Document, len(docs))

    for i := 0; i < len(docs); i += EMBED_BATCH_SIZE {
        end := i + EMBED_BATCH_SIZE
        if end > len(docs) {
            end = len(docs)
        }

        batch := docs[i:end]
        batchSize := len(batch)

        // Prepare texts for embedding
        texts := make([]string, batchSize)
        for j, doc := range batch {
            if doc.ID == "" {
                doc.ID = uuid.New().String()
            }
            doc.CreatedAt = time.Now()
            doc.UpdatedAt = time.Now()

            texts[j] = prepareContentForEmbedding(doc)
        }

        // Single batch API call for all texts
        log.Printf("[VectorRepo] Generating embeddings for batch %d-%d (%d docs)",
            i, end, batchSize)

        embeddings, err := r.generateBatchEmbeddings(ctx, texts)
        if err != nil {
            return fmt.Errorf("batch embedding failed at offset %d: %w", i, err)
        }

        // Build chromem documents with pre-computed embeddings
        for j, doc := range batch {
            metadata := make(map[string]string)
            metadata["content_type"] = string(doc.ContentType)
            metadata["content_id"] = doc.ContentID
            metadata["user_id"] = doc.UserID
            metadata["title"] = doc.Title
            metadata["created_at"] = doc.CreatedAt.Format(time.RFC3339)

            for k, v := range doc.Metadata {
                metadata[k] = v
            }

            chromemDocs[i+j] = chromem.Document{
                ID:        doc.ID,
                Content:   texts[j],
                Embedding: embeddings[j],  // Pre-computed embedding
                Metadata:  metadata,
            }

            r.documentMap[doc.ID] = doc
        }
    }

    // Add all documents to collection (no additional API calls)
    log.Printf("[VectorRepo] Adding %d documents to collection", len(chromemDocs))
    err := r.collection.AddDocuments(ctx, chromemDocs, runtime())
    if err != nil {
        return fmt.Errorf("failed to add documents batch: %w", err)
    }

    now := time.Now()
    r.lastIndexed = &now

    log.Printf("[VectorRepo] Successfully added %d documents in batch", len(docs))
    return nil
}

// New helper method
func (r *VectorRepository) generateBatchEmbeddings(ctx context.Context, texts []string) ([][]float32, error) {
    // Call embedding service batch method
    embeddings := make([][]float32, len(texts))

    for i, text := range texts {
        emb, err := r.embeddingFn(ctx, text)
        if err != nil {
            return nil, fmt.Errorf("embedding failed for text %d: %w", i, err)
        }
        embeddings[i] = emb
    }

    return embeddings, nil
}
```

**Note**: This requires chromem-go to support pre-computed embeddings. Check chromem-go documentation for `Document` struct.

**Alternative**: Modify embedding service to batch calls

**File**: `backend/internal/services/embedding_service.go`

```go
// Add batch method that actually batches API calls
func (s *EmbeddingService) EmbedBatch(ctx context.Context, texts []string) ([][]float32, error) {
    const API_BATCH_SIZE = 200

    allEmbeddings := make([][]float32, len(texts))

    for i := 0; i < len(texts); i += API_BATCH_SIZE {
        end := i + API_BATCH_SIZE
        if end > len(texts) {
            end = len(texts)
        }

        batch := texts[i:end]

        // Clean texts
        cleanedBatch := make([]string, len(batch))
        for j, text := range batch {
            cleanedBatch[j] = s.cleanText(text)
        }

        // Single API call for entire batch
        req := openai.EmbeddingRequest{
            Input: cleanedBatch,  // Array of texts
            Model: s.model,
        }

        resp, err := s.client.CreateEmbeddings(ctx, req)
        if err != nil {
            return nil, fmt.Errorf("batch embedding API call failed: %w", err)
        }

        // Extract embeddings
        for j, embData := range resp.Data {
            allEmbeddings[i+j] = embData.Embedding
        }
    }

    return allEmbeddings, nil
}
```

**Testing**:
```go
// Test batch embedding
texts := []string{"text 1", "text 2", ..., "text 200"}
embeddings, err := embeddingService.EmbedBatch(ctx, texts)
// Verify 200 embeddings returned
// Verify single API call made (check logs)
```

**Expected Impact**:
- 5K memories: 5000 API calls â†’ 25 API calls
- Indexing time: 250s â†’ 20-30s (10-15x faster)
- Cost: 200x reduction in billable API calls

---

#### Day 9: Fix #3 - Relevance Threshold (3 hours)

**File 1**: `backend/internal/models/rag.go`

Add field to SearchRequest:
```go
type SearchRequest struct {
    Query         string   `json:"query" binding:"required"`
    ContentTypes  []string `json:"content_types"`
    Limit         int      `json:"limit"`
    VectorWeight  float64  `json:"vector_weight"`
    MinSimilarity float64  `json:"min_similarity"`  // NEW: 0.0-1.0, default 0.0 (no filtering)
}
```

**File 2**: `backend/internal/services/rag_service.go`

Update Search() method:
```go
func (s *RAGService) Search(ctx context.Context, userID string, req *models.SearchRequest) (*models.SearchResponse, error) {
    startTime := time.Now()

    if req.Limit <= 0 {
        req.Limit = 10
    }
    if req.VectorWeight <= 0 {
        req.VectorWeight = 0.7
    }
    // NEW: Default threshold
    if req.MinSimilarity < 0 {
        req.MinSimilarity = 0.0  // No filtering by default
    }

    log.Printf("[RAG] Hybrid search: user=%s, query=%q, limit=%d, vector_weight=%.2f, min_similarity=%.2f",
        userID, req.Query, req.Limit, req.VectorWeight, req.MinSimilarity)

    // ... existing parallel search code ...

    // Combine results using RRF with threshold
    combined := s.reciprocalRankFusion(vectorResults, keywordResults, req.VectorWeight, req.MinSimilarity)

    // ... rest of method
}
```

Update reciprocalRankFusion():
```go
func (s *RAGService) reciprocalRankFusion(
    vectorResults, keywordResults []models.SearchResult,
    vectorWeight, minSimilarity float64,
) []models.SearchResult {
    const k = 60.0

    scoreMap := make(map[string]float64)
    docMap := make(map[string]*models.SearchResult)

    // Add vector results
    for i, result := range vectorResults {
        key := fmt.Sprintf("%s-%s", result.Document.ContentType, result.Document.ContentID)
        rrf := vectorWeight * (1.0 / (k + float64(i+1)))
        scoreMap[key] += rrf
        if _, exists := docMap[key]; !exists {
            r := result
            r.MatchType = "vector"
            docMap[key] = &r
        }
    }

    // Add keyword results
    keywordWeight := 1.0 - vectorWeight
    for i, result := range keywordResults {
        key := fmt.Sprintf("%s-%s", result.Document.ContentType, result.Document.ContentID)
        rrf := keywordWeight * (1.0 / (k + float64(i+1)))
        scoreMap[key] += rrf

        if existing, exists := docMap[key]; exists {
            existing.MatchType = "hybrid"
            if len(result.Highlights) > 0 {
                existing.Highlights = append(existing.Highlights, result.Highlights...)
            }
        } else {
            r := result
            r.MatchType = "keyword"
            docMap[key] = &r
        }
    }

    // Convert to slice and filter by threshold
    var combined []models.SearchResult
    for key, result := range docMap {
        score := scoreMap[key]
        result.Score = score

        // NEW: Filter by minimum similarity
        if score >= minSimilarity {
            combined = append(combined, *result)
        } else {
            log.Printf("[RAG] Filtered result %s (score %.3f < threshold %.3f)",
                key, score, minSimilarity)
        }
    }

    sort.Slice(combined, func(i, j int) bool {
        return combined[i].Score > combined[j].Score
    })

    log.Printf("[RAG] RRF merged %d results, filtered to %d (threshold: %.2f)",
        len(docMap), len(combined), minSimilarity)

    return combined
}
```

**API Handler Update** (`backend/internal/handlers/rag_handler.go`):
```go
// Allow clients to specify threshold
func (h *RAGHandler) Search(c *gin.Context) {
    var req models.SearchRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(400, gin.H{"error": err.Error()})
        return
    }

    // Default values set in service layer

    resp, err := h.ragService.Search(c.Request.Context(), userID, &req)
    // ...
}
```

**Frontend Update** (`frontend/src/api/rag.ts`):
```typescript
export interface SearchRequest {
  query: string;
  content_types?: string[];
  limit?: number;
  semantic_weight?: number;
  min_similarity?: number;  // NEW: 0.0-1.0
}
```

**Testing**:
```bash
# Test with different thresholds
curl -X POST /api/rag/search \
  -d '{"query": "machine learning", "min_similarity": 0.0}'  # No filtering

curl -X POST /api/rag/search \
  -d '{"query": "machine learning", "min_similarity": 0.5}'  # Medium threshold

curl -X POST /api/rag/search \
  -d '{"query": "machine learning", "min_similarity": 0.7}'  # High threshold
```

**Expected Impact**:
- Precision@10: 0.82 â†’ 0.87-0.92 (+5-10%)
- Fewer irrelevant results
- Tunable per use case

---

#### Day 10: Fix #4 - Parallel Multi-Type Search (3 hours)

**File**: `backend/internal/repository/vector_repository.go:235-271`

**Before** (sequential):
```go
func (r *VectorRepository) SearchByUser(ctx context.Context, userID, query string, limit int, contentTypes []string) ([]models.SearchResult, error) {
    filters := map[string]string{
        "user_id": userID,
    }

    if len(contentTypes) == 1 {
        filters["content_type"] = contentTypes[0]
        return r.Search(ctx, query, limit, filters)
    }

    // SLOW: Sequential queries
    if len(contentTypes) > 1 {
        var allResults []models.SearchResult
        for _, ct := range contentTypes {
            filters["content_type"] = ct
            results, err := r.Search(ctx, query, limit, filters)
            if err != nil {
                return nil, err
            }
            allResults = append(allResults, results...)
        }
        // Sort and limit
        sort.Slice(allResults, func(i, j int) bool {
            return allResults[i].Score > allResults[j].Score
        })
        if len(allResults) > limit {
            allResults = allResults[:limit]
        }
        return allResults, nil
    }

    return r.Search(ctx, query, limit, filters)
}
```

**After** (parallel):
```go
func (r *VectorRepository) SearchByUser(ctx context.Context, userID, query string, limit int, contentTypes []string) ([]models.SearchResult, error) {
    filters := map[string]string{
        "user_id": userID,
    }

    // Single content type - use existing path
    if len(contentTypes) == 1 {
        filters["content_type"] = contentTypes[0]
        return r.Search(ctx, query, limit, filters)
    }

    // Multiple content types - parallel search
    if len(contentTypes) > 1 {
        var mu sync.Mutex
        var allResults []models.SearchResult
        var wg sync.WaitGroup
        var firstError error

        for _, ct := range contentTypes {
            wg.Add(1)
            go func(contentType string) {
                defer wg.Done()

                filters := map[string]string{
                    "user_id": userID,
                    "content_type": contentType,
                }

                results, err := r.Search(ctx, query, limit, filters)
                if err != nil {
                    mu.Lock()
                    if firstError == nil {
                        firstError = err
                    }
                    mu.Unlock()
                    log.Printf("[VectorRepo] Search failed for content type %s: %v", contentType, err)
                    return
                }

                mu.Lock()
                allResults = append(allResults, results...)
                mu.Unlock()

                log.Printf("[VectorRepo] Found %d results for content type %s", len(results), contentType)
            }(ct)
        }

        wg.Wait()

        // Return error if any search failed
        if firstError != nil {
            return nil, firstError
        }

        // Sort by score and limit
        sort.Slice(allResults, func(i, j int) bool {
            return allResults[i].Score > allResults[j].Score
        })

        if len(allResults) > limit {
            allResults = allResults[:limit]
        }

        log.Printf("[VectorRepo] Parallel search complete: %d total results from %d content types",
            len(allResults), len(contentTypes))

        return allResults, nil
    }

    // No content type filter
    return r.Search(ctx, query, limit, filters)
}
```

**Testing**:
```go
// Benchmark parallel vs. sequential
func BenchmarkMultiTypeSearch(b *testing.B) {
    contentTypes := []string{"todo", "memory"}

    b.Run("Sequential", func(b *testing.B) {
        for i := 0; i < b.N; i++ {
            // Old implementation
        }
    })

    b.Run("Parallel", func(b *testing.B) {
        for i := 0; i < b.N; i++ {
            repo.SearchByUser(ctx, userID, query, 10, contentTypes)
        }
    })
}
```

**Expected Impact**:
- 2 content types: 100ms â†’ 50-60ms (40-50% reduction)
- 3+ content types: even larger gains
- Scales with more content types

---

#### Day 11: Fix #5 - Truncation Warnings (1 hour)

**File**: `backend/internal/services/embedding_service.go:94-177`

**Update EmbedBatch()**:
```go
func (s *EmbeddingService) EmbedBatch(ctx context.Context, texts []string) ([][]float32, error) {
    if !s.IsConfigured() {
        return nil, fmt.Errorf("embedding service not configured")
    }

    cleanedTexts := make([]string, len(texts))
    truncationCount := 0
    truncationStats := make(map[string]int)  // Length bucket -> count

    for i, text := range texts {
        // Replace newlines with spaces
        text = strings.ReplaceAll(text, "\n", " ")

        originalLen := len(text)

        // Truncate if needed
        if originalLen > 8000 {
            text = text[:8000]
            truncationCount++

            // Track severity
            loss := originalLen - 8000
            lossPercent := 100.0 * (float64(loss) / float64(originalLen))

            // Categorize by loss percentage
            var bucket string
            switch {
            case lossPercent < 10:
                bucket = "minor (<10%)"
            case lossPercent < 25:
                bucket = "moderate (10-25%)"
            case lossPercent < 50:
                bucket = "significant (25-50%)"
            default:
                bucket = "severe (>50%)"
            }
            truncationStats[bucket]++

            log.Printf("[WARNING] Text %d truncated: %d â†’ 8000 chars (%.1f%% loss, %s)",
                i, originalLen, lossPercent, bucket)
        }

        // Ensure non-empty
        if text == "" {
            text = " "
        }

        cleanedTexts[i] = text
    }

    // Summary logging
    if truncationCount > 0 {
        log.Printf("[WARNING] Truncated %d/%d texts in batch:", truncationCount, len(texts))
        for bucket, count := range truncationStats {
            log.Printf("  - %s: %d texts", bucket, count)
        }

        // Consider alerting if severe truncation
        if truncationStats["severe (>50%)"] > 0 {
            log.Printf("[ALERT] %d texts lost >50%% of content due to truncation",
                truncationStats["severe (>50%)"])
        }
    }

    // ... rest of embedding generation ...
}
```

**Add Metrics Collection** (optional):
```go
// In embedding_service.go
type TruncationMetrics struct {
    TotalTexts      int
    TruncatedTexts  int
    AvgOriginalLen  int
    AvgTruncatedLen int
    MaxLoss         int
}

var globalTruncationMetrics TruncationMetrics

func (s *EmbeddingService) GetTruncationMetrics() TruncationMetrics {
    return globalTruncationMetrics
}
```

**Add Admin Endpoint** (optional):
```go
// In handlers/rag_handler.go
func (h *RAGHandler) GetTruncationStats(c *gin.Context) {
    metrics := h.embeddingService.GetTruncationMetrics()
    c.JSON(200, gin.H{
        "total_texts": metrics.TotalTexts,
        "truncated": metrics.TruncatedTexts,
        "truncation_rate": float64(metrics.TruncatedTexts) / float64(metrics.TotalTexts),
        "avg_original_length": metrics.AvgOriginalLen,
        "avg_truncated_length": metrics.AvgTruncatedLen,
        "max_loss": metrics.MaxLoss,
    })
}
```

**Testing**:
```go
// Test with various content lengths
texts := []string{
    strings.Repeat("a", 100),      // Short
    strings.Repeat("b", 5000),     // Medium
    strings.Repeat("c", 9000),     // Truncated (minor)
    strings.Repeat("d", 20000),    // Truncated (severe)
}

embeddings, err := service.EmbedBatch(ctx, texts)

// Check logs for truncation warnings
// Verify metrics tracking
```

**Expected Impact**:
- Visibility into truncation frequency
- Data quality insights
- Foundation for future chunking implementation

---

### Phase 4: Validation (Week 2, Days 12-14)

#### Day 12: Re-run Benchmarks (6 hours)

**Tasks**:
1. Clear vector database
2. Re-run baseline benchmarks with improvements
3. Collect detailed metrics
4. Compare with original baseline

**Commands**:
```bash
# Clean vector DB
rm -rf ./data/vectors/*

# Re-run benchmarks
./benchmark baseline \
  --dataset fixtures/5k.json \
  --output results/improved_5k.json \
  --verbose

# Generate comparison
./benchmark compare \
  --before results/baseline_5k.json \
  --after results/improved_5k.json \
  --output results/comparison.md \
  --format markdown
```

**Expected Results** (5K memories):
```
Indexing:
  Before: 250-400s
  After:  20-30s
  Improvement: 10-15x faster âœ…

Search Latency (P95):
  Before: 80-150ms
  After:  60-80ms
  Improvement: 25-50% faster âœ…

Precision@10:
  Before: 0.70-0.85
  After:  0.80-0.92
  Improvement: +5-10% âœ…

API Calls (5K):
  Before: 5,000
  After:  25
  Reduction: 200x âœ…
```

---

#### Day 13: Analysis & Documentation (10 hours)

**Tasks**:
1. Validate improvement predictions vs. actual
2. Investigate any regressions
3. Document lessons learned
4. Identify next optimization targets
5. Update this guide with findings

**Analysis Questions**:
- Did batch embedding achieve 10-15x speedup?
- Did parallel search reduce latency by 40-60%?
- Did threshold filtering improve precision?
- Were there any unexpected side effects?
- What's the next biggest bottleneck?

**Documentation Updates**:
- Add actual benchmark results
- Update expected metrics with real data
- Document any deviations from plan
- Add troubleshooting section

---

#### Day 14: Final Report & Handoff (4 hours)

**Deliverables**:
1. Executive summary (1 page)
2. Detailed comparison report (markdown)
3. Charts/graphs (if tooling permits)
4. Recommendations for future work
5. Code review and cleanup

**Report Structure**:
```markdown
# RAG System Optimization Report

## Executive Summary
- Baseline results
- Improvements implemented
- Performance gains achieved
- Cost savings
- Next steps

## Detailed Results
### Performance Improvements
[Tables with before/after]

### Accuracy Improvements
[Precision/recall comparisons]

### Cost Analysis
[API call reduction, savings]

## Lessons Learned
- What worked well
- Unexpected challenges
- Best practices discovered

## Future Recommendations
1. Implement chunking for long content
2. Evaluate ANN-based vector DB
3. Add query caching
4. Implement reranking layer
5. Monitor long-term performance

## Appendices
- Full benchmark results (JSON)
- Test datasets
- Code changes summary
```

---

## Optimization Strategies

### Implemented (Week 2)

#### 1. Batch Embedding API Calls âš¡

**Impact**: 10-15x indexing speedup

**Approach**:
- Change from individual API calls to batched calls
- Batch size: 200 texts per API request
- Pre-compute embeddings before adding to chromem-go

**Benefits**:
- 5K memories: 5000 calls â†’ 25 calls (200x reduction)
- Reduced network overhead
- Lower API costs (20x fewer billable requests)
- Faster indexing (250s â†’ 20-30s)

**Trade-offs**:
- None - pure performance gain
- Requires code changes in vector repository

---

#### 2. Remove 1000 Memory Limit ðŸš¨

**Impact**: Unblocks target scale

**Approach**:
- Replace hard-coded limit with pagination
- Batch size: 1000 memories per fetch
- Loop until no more memories

**Benefits**:
- Enables 5K-10K scale testing
- Supports unlimited memory count
- Incremental indexing possible

**Trade-offs**:
- None - critical bug fix

---

#### 3. Relevance Score Thresholding ðŸŽ¯

**Impact**: +5-10% precision improvement

**Approach**:
- Add `MinSimilarity` field to SearchRequest
- Filter results in RRF by threshold
- Default: 0.0 (no filtering for backward compatibility)

**Benefits**:
- Precision@10: 0.82 â†’ 0.87-0.92
- Fewer irrelevant results
- Better user experience
- Tunable per use case

**Trade-offs**:
- May reduce recall slightly
- Requires API change (backward compatible)

**Recommended Thresholds**:
- 0.0: No filtering (default)
- 0.3: Very permissive
- 0.5: Moderate filtering
- 0.7: Strict filtering
- 0.9: Only near-exact matches

---

#### 4. Parallel Multi-Type Search âš¡

**Impact**: 40-60% latency reduction

**Approach**:
- Launch goroutines for each content type
- Run searches in parallel
- Merge and sort results

**Benefits**:
- 2 types: 100ms â†’ 50-60ms
- 3+ types: even larger gains
- Better scalability

**Trade-offs**:
- Slightly more complex code
- Higher CPU utilization during search
- Requires thread-safe result merging

---

#### 5. Truncation Warnings ðŸ“Š

**Impact**: Observability improvement

**Approach**:
- Log when text is truncated
- Track truncation statistics
- Categorize by severity

**Benefits**:
- Visibility into data quality
- Foundation for chunking implementation
- Alerts for severe truncation

**Trade-offs**:
- None - pure logging addition

---

### Future Enhancements

#### 1. Smart Chunking with Overlap ðŸ“„

**Priority**: HIGH
**Complexity**: MEDIUM (4-6 hours)
**Impact**: +10-15% recall for long documents

**Approach**:
```go
func chunkWithOverlap(text string, chunkSize, overlap int) []string {
    if len(text) <= chunkSize {
        return []string{text}
    }

    chunks := []string{}
    start := 0

    for start < len(text) {
        end := start + chunkSize
        if end > len(text) {
            end = len(text)
        }

        chunks = append(chunks, text[start:end])

        // Move forward by (chunkSize - overlap)
        start += (chunkSize - overlap)
    }

    return chunks
}
```

**Configuration**:
- Chunk size: 4000 characters
- Overlap: 2000 characters (50%)
- Max pooling for chunk embeddings

**Benefits**:
- No content loss for long documents
- Better context preservation
- Improved recall

**Challenges**:
- Storage increase (2-3x for long docs)
- More embeddings to generate
- Chunk attribution complexity

---

#### 2. Query Result Caching ðŸ’¾

**Priority**: MEDIUM
**Complexity**: LOW (2-3 hours)
**Impact**: 99% hit rate for common queries

**Approach**:
```go
type QueryCache struct {
    cache *lru.Cache  // LRU cache
    mu    sync.RWMutex
    ttl   time.Duration
}

func (c *QueryCache) Get(key string) (*models.SearchResponse, bool) {
    c.mu.RLock()
    defer c.mu.RUnlock()

    if val, ok := c.cache.Get(key); ok {
        cached := val.(*CachedResult)
        if time.Since(cached.Timestamp) < c.ttl {
            return cached.Response, true
        }
        c.cache.Remove(key)
    }
    return nil, false
}

func (c *QueryCache) Set(key string, resp *models.SearchResponse) {
    c.mu.Lock()
    defer c.mu.Unlock()

    c.cache.Add(key, &CachedResult{
        Response:  resp,
        Timestamp: time.Now(),
    })
}
```

**Configuration**:
- Cache size: 1000 queries
- TTL: 5 minutes
- Key: `hash(userID + query + contentTypes + vectorWeight)`

**Benefits**:
- Sub-millisecond response for cached queries
- Reduced load on vector DB
- Lower API costs

**Challenges**:
- Cache invalidation on new memories
- Memory usage
- Stale results risk

---

#### 3. ANN-Based Vector Database ðŸš€

**Priority**: HIGH (for >10K scale)
**Complexity**: HIGH (1-2 weeks)
**Impact**: 10-100x search speedup at large scale

**Options**:
1. **Qdrant** (Rust-based, best performance)
2. **Weaviate** (Go-native, good ecosystem)
3. **Milvus** (Comprehensive, complex)
4. **FAISS** (Library, requires wrapper)

**Comparison**:

| Feature | chromem-go | Qdrant | Weaviate | Milvus |
|---------|-----------|--------|----------|--------|
| Search Algo | Linear O(n) | HNSW O(log n) | HNSW | Multiple |
| Language | Go | Rust | Go | C++/Python |
| 10K Search | 120-200ms | 5-15ms | 10-25ms | 5-20ms |
| Scaling | Poor | Excellent | Good | Excellent |
| Complexity | Simple | Medium | Medium | High |

**Recommendation**: **Qdrant** for best performance, **Weaviate** for Go ecosystem

**Migration Path**:
```go
// 1. Add abstraction layer
type VectorStore interface {
    Add(ctx context.Context, doc *Document) error
    Search(ctx context.Context, query string, limit int) ([]SearchResult, error)
    // ... other methods
}

// 2. Implement for chromem-go (existing)
type ChromemVectorStore struct { ... }

// 3. Implement for Qdrant
type QdrantVectorStore struct { ... }

// 4. Feature flag for gradual rollout
```

---

#### 4. Semantic Query Expansion ðŸ”

**Priority**: MEDIUM
**Complexity**: MEDIUM (3-4 hours)
**Impact**: +5-10% recall for ambiguous queries

**Approach**:
```go
func (s *RAGService) expandQuery(ctx context.Context, query string) []string {
    // Use LLM to generate query variations
    prompt := fmt.Sprintf(`Generate 3 alternative phrasings of this query:

Query: %s

Return as JSON array: ["variation 1", "variation 2", "variation 3"]`, query)

    variations, err := s.aiService.Complete(ctx, prompt)
    if err != nil {
        return []string{query}  // Fallback to original
    }

    return append([]string{query}, variations...)
}

func (s *RAGService) SearchWithExpansion(ctx context.Context, userID string, req *models.SearchRequest) (*models.SearchResponse, error) {
    // Expand query
    queries := s.expandQuery(ctx, req.Query)

    // Search with each variation
    var allResults []models.SearchResult
    for _, q := range queries {
        req.Query = q
        resp, err := s.Search(ctx, userID, req)
        if err == nil {
            allResults = append(allResults, resp.Results...)
        }
    }

    // Deduplicate and re-rank
    return s.deduplicateAndRank(allResults, req.Limit)
}
```

**Benefits**:
- Better handling of synonyms
- More robust to query phrasing
- Increased recall

**Challenges**:
- Additional LLM API calls (cost)
- Increased latency
- May dilute precision

---

#### 5. Cross-Encoder Reranking ðŸŽ¯

**Priority**: LOW (nice-to-have)
**Complexity**: HIGH (1 week)
**Impact**: +2-5% precision improvement

**Approach**:
```go
// 1. First stage: Fast retrieval (current hybrid search)
candidates := s.Search(ctx, userID, req)  // Get top 50

// 2. Second stage: Precise reranking with cross-encoder
reranked := s.rerank(ctx, req.Query, candidates.Results[:50])

// 3. Return top K from reranked results
return reranked[:req.Limit]
```

**Model Options**:
- `cross-encoder/ms-marco-MiniLM-L-6-v2` (fast, good)
- `cross-encoder/ms-marco-electra-base` (slower, better)

**Benefits**:
- Higher precision for top results
- Better relevance scores

**Challenges**:
- Requires ML model hosting
- Increased latency (50-200ms)
- Infrastructure complexity

---

## Expected Performance Gains

### Baseline (Before Optimizations)

**Dataset**: 5,000 memories, 100 queries

| Metric | Value | Unit |
|--------|-------|------|
| **Indexing** | | |
| Total Time | 250-400 | seconds |
| Throughput | 12-20 | docs/sec |
| API Calls | 5,000 | calls |
| **Search** | | |
| P50 Latency | 45-70 | ms |
| P95 Latency | 80-150 | ms |
| P99 Latency | 120-200 | ms |
| **Accuracy** | | |
| Precision@10 | 0.70-0.85 | - |
| Recall@10 | 0.60-0.75 | - |
| MRR | 0.65-0.80 | - |

---

### Post-Optimization (After Week 2)

**Dataset**: 5,000 memories, 100 queries

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Indexing** | | | |
| Total Time | 275s | 25s | **11x faster** âš¡ |
| Throughput | 18 docs/s | 200 docs/s | **11x faster** âš¡ |
| API Calls | 5,000 | 25 | **200x fewer** ðŸ’° |
| **Search (2 types)** | | | |
| P95 Latency | 115ms | 68ms | **41% faster** âš¡ |
| P99 Latency | 160ms | 95ms | **41% faster** âš¡ |
| **Accuracy** | | | |
| Precision@10 | 0.77 | 0.89 | **+16%** ðŸŽ¯ |
| Recall@10 | 0.67 | 0.71 | **+6%** ðŸŽ¯ |
| MRR | 0.72 | 0.83 | **+15%** ðŸŽ¯ |

---

### Cost Analysis

**Scenario**: 10 users, 5,000 memories each, re-index monthly

**Before Optimizations**:
```
Embedding API calls per user:  5,000
Total API calls per month:     10 Ã— 5,000 = 50,000
Tokens per memory (avg):       200
Total tokens:                  50,000 Ã— 200 = 10M tokens
Cost (text-embedding-3-small): $0.02 / 1M tokens
Monthly cost:                  10 Ã— $0.02 = $0.20
```

**After Optimizations**:
```
Embedding API calls per user:  25 (batched)
Total API calls per month:     10 Ã— 25 = 250
Total tokens:                  10M tokens (same)
Cost:                          $0.20 (same)
Time saved:                    10 Ã— (250s - 25s) = 37.5 minutes/month
```

**Key Insight**: Cost is based on tokens, not API calls. Batching doesn't reduce cost but **dramatically** improves performance.

**Search Cost Savings** (with caching):
```
Searches per user per day:     50
Cache hit rate:                90%
API-based searches:            50 Ã— 0.1 = 5 per day
Without cache:                 50 per day
Reduction:                     90%
```

---

## Technical Specifications

### System Requirements

**Development**:
- Go 1.21+
- 4 GB RAM minimum
- 10 GB disk space
- OpenAI API key

**Production (5K memories)**:
- Go 1.21+
- 8 GB RAM recommended
- 20 GB disk space
- OpenAI API key
- PostgreSQL/SQLite

**Production (50K memories)**:
- Go 1.21+
- 16 GB RAM
- 100 GB disk space
- Consider ANN vector DB (Qdrant/Weaviate)

---

### Dependencies

**Core**:
```bash
go get github.com/philippgille/chromem-go@v0.7.0  # Vector DB
go get github.com/sashabaranov/go-openai         # OpenAI client
```

**Benchmark Suite**:
```bash
go get github.com/brianvoe/gofakeit/v6           # Fake data
go get github.com/montanaflynn/stats             # Statistics
go get github.com/spf13/cobra                    # CLI framework
go get github.com/spf13/viper                    # Configuration
go get github.com/olekukonko/tablewriter         # Tables
go get github.com/schollz/progressbar/v3         # Progress bars
```

**Testing**:
```bash
go get github.com/stretchr/testify               # Test assertions
```

---

### Configuration

**Environment Variables**:
```bash
# OpenAI
OPENAI_API_KEY=sk-...
OPENAI_BASE_URL=https://api.openai.com/v1
EMBEDDING_MODEL=text-embedding-3-small

# Vector DB
VECTOR_DB_PATH=./data/vectors
RAG_ENABLED=true

# Database
DATABASE_PATH=./data/todomyday.db

# Benchmark
BENCHMARK_OUTPUT_DIR=./benchmark_results
BENCHMARK_FIXTURES_DIR=./internal/benchmark/fixtures
```

---

## Testing & Validation

### Unit Tests

```bash
# Test metric calculations
go test ./internal/benchmark -run TestPrecisionRecall
go test ./internal/benchmark -run TestMRR
go test ./internal/benchmark -run TestNDCG

# Test data generation
go test ./internal/benchmark -run TestGenerateDataset

# Test performance benchmarks
go test ./internal/benchmark -run TestBenchmarkIndexing
```

---

### Integration Tests

```bash
# Full benchmark suite
go test ./internal/benchmark -run TestFullBenchmark -timeout 30m

# Compare results
go test ./internal/benchmark -run TestCompareResults
```

---

### Manual Testing

```bash
# 1. Generate small dataset for quick iteration
./benchmark generate --size 100 --output fixtures/test.json

# 2. Run baseline
./benchmark baseline --dataset fixtures/test.json --output results/test.json

# 3. Inspect results
cat results/test.json | jq .performance

# 4. Generate report
./benchmark baseline --dataset fixtures/test.json --export markdown

# 5. Test specific queries
curl -X POST localhost:8080/api/rag/search \
  -d '{"query": "machine learning", "limit": 10}'
```

---

## Future Enhancements

### Short-term (1-2 months)

1. **Query Caching** (2-3 hours)
   - LRU cache for frequent queries
   - 99% hit rate potential
   - Sub-ms response time

2. **Incremental Indexing** (3-4 hours)
   - Only re-index changed memories
   - Track last update timestamp
   - Faster updates

3. **Admin Dashboard** (1 week)
   - Real-time indexing stats
   - Search performance metrics
   - Truncation statistics
   - Cost tracking

4. **A/B Testing Framework** (3-4 days)
   - Test different RRF weights
   - Compare search algorithms
   - Measure user satisfaction

---

### Medium-term (3-6 months)

1. **ANN Vector Database** (2-3 weeks)
   - Migrate to Qdrant or Weaviate
   - 10-100x search speedup
   - Better scaling to 100K+ memories

2. **Semantic Chunking** (1 week)
   - Split long documents intelligently
   - Sentence-level chunking
   - Paragraph-aware splits

3. **Multi-Vector Search** (2 weeks)
   - Separate embeddings for title/content
   - Query-specific weighting
   - Better result ranking

4. **User Feedback Loop** (1 week)
   - Thumbs up/down on results
   - Click tracking
   - Personalized ranking

---

### Long-term (6-12 months)

1. **Distributed Search** (1-2 months)
   - Shard indices across nodes
   - Support millions of memories
   - Multi-region deployment

2. **Advanced Reranking** (3-4 weeks)
   - Cross-encoder models
   - Learning-to-rank
   - Personalization

3. **Multi-Modal Search** (2-3 months)
   - Image embeddings
   - PDF/document parsing
   - Code understanding

4. **Conversational Search** (1 month)
   - Multi-turn queries
   - Context preservation
   - Query refinement

---

## Appendices

### A. Glossary

**Precision@K**: Fraction of top K results that are relevant
**Recall@K**: Fraction of all relevant docs found in top K
**MRR**: Mean Reciprocal Rank - average of 1/position_of_first_relevant
**NDCG**: Normalized Discounted Cumulative Gain - quality-weighted ranking
**RRF**: Reciprocal Rank Fusion - method for combining search results
**ANN**: Approximate Nearest Neighbors - fast similarity search
**HNSW**: Hierarchical Navigable Small World - popular ANN algorithm
**Embedding**: Vector representation of text (1536 dimensions)
**FTS5**: SQLite Full-Text Search version 5
**chromem-go**: Go-native vector database library

---

### B. References

**Research Papers**:
- [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)
- [HNSW Algorithm](https://arxiv.org/abs/1603.09320)
- [NDCG Metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)

**Documentation**:
- [chromem-go GitHub](https://github.com/philippgille/chromem-go)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
- [SQLite FTS5](https://www.sqlite.org/fts5.html)

**Tools**:
- [Vector DB Benchmark](https://github.com/qdrant/vector-db-benchmark)
- [Information Retrieval Metrics](https://github.com/yashLadha/ir-measures)

---

### C. Troubleshooting

**Issue**: Indexing is slow even with batching
**Solution**: Check EMBED_BATCH_SIZE, verify API calls are actually batched (check logs), ensure network latency is not the issue

**Issue**: Search returns no results
**Solution**: Verify vector DB has documents (`collection.Count()`), check user_id filter, ensure embeddings were generated correctly

**Issue**: Precision is low
**Solution**: Increase MinSimilarity threshold, improve query quality, validate ground truth labels, check embedding model quality

**Issue**: Memory usage too high
**Solution**: Enable persistent storage, reduce in-memory cache size, consider lazy loading, use pagination for large collections

**Issue**: Truncation warnings constantly appearing
**Solution**: Implement chunking with overlap, increase 8000 char limit (requires OpenAI model change), pre-process content to remove boilerplate

---

### D. Contact & Support

**Questions**: Create GitHub issue at repo
**Documentation**: See `/docs/` folder
**Benchmark Results**: Check `/benchmark_results/` folder
**Code**: `backend/internal/benchmark/`, `backend/cmd/benchmark/`

---

**Document End**
